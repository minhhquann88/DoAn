import requests
import json
import time
import logging
from typing import List, Dict, Any

# --- Cáº¤U HÃŒNH LOGGING ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - [F8-CRAWLER] - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

class F8FinalCrawler:
    def __init__(self):
        self.api_url = "https://api-gateway.f8.edu.vn/api/combined-courses"
        
        # User-Agent báº¡n Ä‘Ã£ cung cáº¥p
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36',
            'Referer': 'https://f8.edu.vn/',
            'Origin': 'https://f8.edu.vn',
            'Accept': 'application/json, text/plain, */*',
        }

    def fetch_data(self) -> List[Dict]:
        logger.info(f"ğŸš€ Äang káº¿t ná»‘i tá»›i API: {self.api_url}")
        try:
            time.sleep(1)
            response = requests.get(self.api_url, headers=self.headers, timeout=15)
            
            if response.status_code != 200:
                logger.error(f"âŒ Lá»—i HTTP: {response.status_code}")
                return []
            
            data = response.json()
            return self.process_data(data)
            
        except Exception as e:
            logger.error(f"âŒ Lá»—i: {e}")
            return []

    def process_data(self, api_response: Dict) -> List[Dict]:
        """
        Xá»­ lÃ½ cáº¥u trÃºc:
        {
            "free_courses": { "data": [...] },
            "pro_courses": { "data": [...] }
        }
        """
        all_courses = []
        
        # Duyá»‡t qua cÃ¡c key chÃ­nh (free_courses, pro_courses)
        for category_key, category_value in api_response.items():
            # Kiá»ƒm tra xem value cÃ³ pháº£i dict vÃ  cÃ³ chá»©a 'data' khÃ´ng
            if isinstance(category_value, dict) and 'data' in category_value:
                courses_list = category_value['data']
                
                # XÃ¡c Ä‘á»‹nh tÃªn nhÃ³m dá»±a trÃªn key
                group_name = "Miá»…n phÃ­" if "free" in category_key else "Pro/Tráº£ phÃ­"
                
                logger.info(f"ğŸ“‚ Äang xá»­ lÃ½ nhÃ³m '{category_key}': TÃ¬m tháº¥y {len(courses_list)} khÃ³a.")
                
                for course in courses_list:
                    # TrÃ­ch xuáº¥t dá»¯ liá»‡u sáº¡ch Ä‘á»ƒ import Database
                    clean_course = {
                        'f8_id': course.get('id'),
                        'title': course.get('title'),
                        'slug': course.get('slug'),
                        'description': course.get('description'),
                        'price': course.get('price', 0),
                        'old_price': course.get('old_price', 0),
                        'is_pro': course.get('is_pro', False),
                        'students_count': course.get('students_count', 0),
                        'duration_text': course.get('duration_text', ''),
                        'image_url': course.get('image_url'),
                        'group': group_name
                    }
                    all_courses.append(clean_course)
        
        return all_courses

    def save_to_json(self, data: List[Dict], filename='f8_courses_final.json'):
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        logger.info(f"ğŸ’¾ ÄÃ£ lÆ°u {len(data)} khÃ³a há»c vÃ o file: {filename}")

    def generate_sql(self, data: List[Dict], filename='import_f8.sql'):
        """Táº¡o file SQL Ä‘á»ƒ import vÃ o database luÃ´n cho tiá»‡n"""
        sql_lines = []
        sql_lines.append("INSERT INTO courses (title, slug, price, description, image_url, category) VALUES")
        
        values = []
        for c in data:
            # Escape dáº¥u nhÃ¡y Ä‘Æ¡n Ä‘á»ƒ trÃ¡nh lá»—i SQL
            title = c['title'].replace("'", "''")
            desc = c['description'].replace("'", "''") if c['description'] else ""
            slug = c['slug']
            price = c['price']
            img = c['image_url']
            cat = c['group']
            
            val = f"('{title}', '{slug}', {price}, '{desc}', '{img}', '{cat}')"
            values.append(val)
        
        sql_lines.append(",\n".join(values) + ";")
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("\n".join(sql_lines))
        logger.info(f"ğŸ’¾ ÄÃ£ táº¡o file SQL import: {filename}")

if __name__ == "__main__":
    crawler = F8FinalCrawler()
    
    # 1. Crawl
    results = crawler.fetch_data()
    
    if results:
        # 2. In thá»­ káº¿t quáº£ ra mÃ n hÃ¬nh
        print("\n" + "="*60)
        print(f"ğŸ‰ Tá»”NG Há»¢P: {len(results)} KHÃ“A Há»ŒC")
        print("="*60)
        print(f"{'TÃŠN KHÃ“A Há»ŒC':<40} | {'GIÃ':<10} | {'LOáº I'}")
        print("-" * 65)
        
        for c in results:
            price_str = f"{c['price']:,}" if c['price'] > 0 else "Free"
            print(f"{c['title'][:37]+'...':<40} | {price_str:<10} | {c['group']}")
            
        # 3. LÆ°u JSON
        crawler.save_to_json(results)
        
        # 4. Táº¡o luÃ´n SQL (Bonus)
        crawler.generate_sql(results)